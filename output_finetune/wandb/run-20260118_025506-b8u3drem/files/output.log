✅ Wandb initialized: project=watermark-anything, name=output_finetune
making attention of type 'vanilla' with 64 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 64 in_channels
VAEEmbedder(
  (encoder): VAEEncoder(
    (conv_in): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (down): ModuleList(
      (0-2): 3 x Module(
        (block): ModuleList(
          (0-1): 2 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (2-3): 2 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
            )
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (downsample): Downsample(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2))
        )
      )
      (3): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
          )
          (1): ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
          (2-3): 2 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
            )
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
    )
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 64, eps=1e-06, affine=True)
        (q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (norm_out): GroupNorm(32, 64, eps=1e-06, affine=True)
    (conv_out): Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (decoder): VAEDecoder(
    (conv_in): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (mid): Module(
      (block_1): ResnetBlock(
        (norm1): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (attn_1): AttnBlock(
        (norm): GroupNorm(32, 64, eps=1e-06, affine=True)
        (q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (k): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (v): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (proj_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      )
      (block_2): ResnetBlock(
        (norm1): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (norm2): ConditionalNorm(
          (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
          (embed): Linear(in_features=128, out_features=128, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (up): ModuleList(
      (0): Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
      )
      (1): Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): Module(
        (block): ModuleList(
          (0): ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (nin_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
          )
          (1-2): 2 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 32, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=64, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (3): Module(
        (block): ModuleList(
          (0-2): 3 x ResnetBlock(
            (norm1): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (norm2): ConditionalNorm(
              (norm): GroupNorm(32, 64, eps=1e-06, affine=False)
              (embed): Linear(in_features=128, out_features=128, bias=True)
            )
            (dropout): Dropout(p=0.0, inplace=False)
            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (attn): ModuleList()
        (upsample): Upsample(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (norm_out): GroupNorm(32, 32, eps=1e-06, affine=True)
    (conv_out): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (msg_processor): MsgProcessor(
    (msg_embeddings): Embedding(64, 64)
  )
  (msg_mlp): Sequential(
    (0): Linear(in_features=32, out_features=64, bias=True)
    (1): SiLU()
    (2): Linear(in_features=64, out_features=128, bias=True)
  )
)
embedder: 1.9M parameters
augmenter: Augmenter(augs=['Identity', 'JPEG', 'Resize', 'Crop', 'Rotate', 'HorizontalFlip', 'Perspective', 'GaussianBlur', 'MedianFilter', 'Brightness', 'Contrast', 'Saturation', 'Hue', 'CropResizePad'], probs=tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,
        0.0667, 0.0667, 0.0667, 0.0667, 0.1333]))
extractor: 93.3M parameters
attenuation: None
Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]
/DATA/guantianrui/anaconda3/envs/watermark_anything/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/DATA/guantianrui/anaconda3/envs/watermark_anything/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Loading model from: /DATA/guantianrui/anaconda3/envs/watermark_anything/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth
LPIPSWithDiscriminator(
  (perceptual_loss): PerceptualLoss(percep_loss=lpips)
  (discriminator): NLayerDiscriminator(
    (main): Sequential(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.2, inplace=True)
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (3): GroupNorm(4, 64, eps=1e-05, affine=True)
      (4): LeakyReLU(negative_slope=0.2, inplace=True)
      (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
      (6): GroupNorm(4, 128, eps=1e-05, affine=True)
      (7): LeakyReLU(negative_slope=0.2, inplace=True)
      (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
    )
  )
  (detection_loss): BCEWithLogitsLoss()
  (decoding_loss): BCEWithLogitsLoss()
)
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.0001
    lr: 1e-06
    weight_decay: 0.01
)
scheduler: <timm.scheduler.cosine_lr.CosineLRScheduler object at 0x7ffa00231ff0>
optimizer_d: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.01
)
/home/guantianrui/watermark-anything/train.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler() if use_amp else None
Using Automatic Mixed Precision (AMP) for training
loading annotations into memory...
Done (t=13.87s)
creating index...
index created!
loading annotations into memory...
Done (t=0.41s)
creating index...
index created!
loading annotations into memory...
Done (t=0.34s)
creating index...
index created!
Restoring Extractor weights from /home/guantianrui/watermark-anything/checkpoints/wam_mit.pth...
✅ Successfully loaded 188 keys (Extractor & Augmenter).
ℹ️  Missing keys (Expected, as Embedder is initialized from scratch): 277
training...
❄️  Warm-up: Freezing Extractor for 5 epochs.
Epoch 0 - scaling_w: 1.0
/home/guantianrui/watermark-anything/train.py:571: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=use_amp):
Train - Epoch: [0/100]  [  0/924]  eta: 7:51:06  total_loss: 37.023441 (37.023441)  percep_loss: 0.211670 (0.211670)  percep_scale: 2.106511 (2.106511)  disc_loss: 0.343506 (0.343506)  disc_scale: 1.412666 (1.412666)  detect_loss: 2.953262 (2.953262)  detect_scale: 0.080000 (0.080000)  decode_loss: 0.704073 (0.704073)  decode_scale: 50.926567 (50.926567)  psnr: 24.860189 (24.860189)  lr: 0.000001 (0.000001)  avg_target: 0.616962 (0.616962)  acc: 0.399216 (0.399216)  iou_0: 0.388060 (0.388060)  iou_1: 0.031888 (0.031888)  avg_pred: -5.816406 (-5.816406)  norm_avg: 6940.000000 (6940.000000)  miou: 0.209974 (0.209974)  bit_acc: 0.520881 (0.520881)  time: 30.591341  data: 2.165563  max mem: 17855
/DATA/guantianrui/anaconda3/envs/watermark_anything/lib/python3.10/site-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Train - Epoch: [0/100]  [ 10/924]  eta: 3:27:36  total_loss: 10.076942 (30.258673)  percep_loss: 0.136353 (0.141996)  percep_scale: 0.490941 (1.451435)  disc_loss: -0.099792 (-0.026534)  disc_scale: 0.034648 (0.299410)  detect_loss: 2.546438 (1.917411)  detect_scale: 0.080000 (0.058182)  decode_loss: 0.704073 (0.246356)  decode_scale: 13.502522 (40.843796)  psnr: 26.263859 (26.327440)  lr: 0.000001 (0.000001)  avg_target: 0.424911 (0.365016)  acc: 0.586920 (0.650792)  iou_0: 0.581970 (0.638207)  iou_1: 0.016007 (0.028153)  avg_pred: -6.507812 (-6.847124)  norm_avg: 8060.000000 (7672.000000)  miou: 0.305466 (0.333180)  bit_acc: nan (0.506205)  time: 13.628980  data: 0.197080  max mem: 17953
Train - Epoch: [0/100]  [ 20/924]  eta: 2:08:57  total_loss: 5.279400 (69127429000.794739)  percep_loss: 0.138428 (0.148307)  percep_scale: 0.490941 (1.534858)  disc_loss: -0.038574 (0.057269)  disc_scale: 0.021217 (0.182235)  detect_loss: 1.266220 (1.966386)  detect_scale: 0.080000 (0.060952)  decode_loss: 0.704776 (0.306350)  decode_scale: 6.781584 (98124150528.466919)  psnr: 26.263859 (26.293851)  lr: 0.000001 (0.000001)  avg_target: 0.272940 (0.357146)  acc: 0.737379 (0.659321)  iou_0: 0.729629 (0.649207)  iou_1: 0.016007 (0.046204)  avg_pred: -7.933594 (-6.999349)  norm_avg: 8688.000000 (7805.333333)  miou: 0.428085 (0.347705)  bit_acc: nan (0.502824)  time: 7.457628  data: 0.000211  max mem: 17953
Train - Epoch: [0/100]  [ 30/924]  eta: 1:28:39  total_loss: 5.279400 (302048730964.547546)  percep_loss: 0.136353 (0.141127)  percep_scale: 0.805720 (1.785709)  disc_loss: 0.303955 (0.153833)  disc_scale: 0.021217 (0.137170)  detect_loss: 1.580567 (2.055088)  detect_scale: 0.080000 (0.061935)  decode_loss: 0.708233 (0.328550)  decode_scale: 6.954096 (422746170795.296997)  psnr: 26.547676 (26.427284)  lr: 0.000001 (0.000001)  avg_target: 0.300705 (0.384989)  acc: 0.733868 (0.633266)  iou_0: 0.727630 (0.621542)  iou_1: 0.030620 (0.046934)  avg_pred: -6.609375 (-6.661227)  norm_avg: 7468.000000 (7486.709677)  miou: 0.394041 (0.334238)  bit_acc: nan (0.498490)  time: 1.726436  data: 0.000193  max mem: 17953
Train - Epoch: [0/100]  [ 40/924]  eta: 1:16:41  total_loss: 5.173755 (542088565522.356018)  percep_loss: 0.136353 (0.142332)  percep_scale: 0.903753 (1.656924)  disc_loss: 0.331543 (0.228976)  disc_scale: 0.015790 (0.108751)  detect_loss: 1.691576 (2.023778)  detect_scale: 0.080000 (0.062439)  decode_loss: 0.711793 (0.339475)  decode_scale: 6.954096 (743202079241.652100)  psnr: 26.636372 (26.443077)  lr: 0.000001 (0.000001)  avg_target: 0.295737 (0.380903)  acc: 0.687809 (0.642509)  iou_0: 0.680459 (0.625406)  iou_1: 0.038244 (0.056158)  avg_pred: -5.980469 (-6.650962)  norm_avg: 6960.000000 (7523.414634)  miou: 0.369482 (0.340782)  bit_acc: nan (0.499393)  time: 1.683288  data: 0.000195  max mem: 17953
